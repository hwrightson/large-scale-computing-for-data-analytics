{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U tensorflow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 11:06:23.673043: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and prepare the MNIST dataset\n",
    "\n",
    "MNIST data is a collection of hand-written digits that contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 255. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Convert to float32\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "\n",
    "# Flatten images to vectors of length 784 (28*28)\n",
    "x_train, x_test = x_train.reshape([-1, 784]), x_test.reshape([-1, 784])\n",
    "\n",
    "# Normalize the pixel intensities to make sure their values are between 0 to 1\n",
    "# by dividing them by 255\n",
    "x_train, x_test = x_train / 255., x_test / 255."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set up hyperparameters etc.\n",
    "\n",
    "- num_classes denotes the number of outputs, which is 10, as we have digits from 0 to 9 in the data set. \n",
    "- num_features defines the number of input parameters, and we store 784 since each image contains 784 pixels.\n",
    "\n",
    "- learning_rate defines the step size the model should take to converge to a minimum loss\n",
    "- training_steps defines the number of steps the model will take to train itself completely\n",
    "- batch_size denotes the number of samples per each batch in the training process\n",
    "- display_step to iterate over the training steps and print them in the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset parameters\n",
    "num_classes = 10\n",
    "num_features = 784\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.01\n",
    "training_steps = 1000\n",
    "batch_size = 256\n",
    "display_step = 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Shuffle and batch data\n",
    "\n",
    "Shuffle and batch the data before we start the actual training to avoid the model from getting biased by the data. This will allow our data to be more random and helps our model to gain higher accuracies with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the slices of an array in the form of objects\n",
    "train_data=tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "\n",
    "# Shuffle and batch the data\n",
    "train_data=train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initialise weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight of shape [784, 10], the 28*28 image features, and a total number of classes.\n",
    "W = tf.Variable(tf.ones([num_features, num_classes]), name=\"weight\")\n",
    "\n",
    "# Bias of shape [10], the total number of classes.\n",
    "b = tf.Variable(tf.zeros([num_classes]), name=\"bias\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Define logistic regression and cost function\n",
    "\n",
    "1. Define a logistic regression function which converts the inputs into a probability distribution proportional to the exponents of the inputs using the softmax function\n",
    "2. Encode the outputs using the function tf.one_hot. We also define and compute the cross-entropy function as the loss function, which is given as cross-entropy loss = -ytrue*(log(ypred)) using tf.reduce_mean and tf.reduce_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(x):\n",
    "    return tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    y_true = tf.one_hot(y_true, depth=num_classes)\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
    "    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Define accuracy calculator and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "optimizer = tf.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Optimization process\n",
    "\n",
    "1. Define a run_optimization() function where we update the weights of our model. \n",
    "    - Calculate the predictions using the logistic_regression(x) method by taking the inputs and find out the loss generated by comparing the predicted value and the original value present in the data set. \n",
    "    - Compute the gradients using and update the weights of the model with our stochastic gradient descent optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(x, y):\n",
    "# Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = logistic_regression(x)\n",
    "        loss = cross_entropy(pred, y)\n",
    "    # Compute gradients.\n",
    "    gradients = g.gradient(loss, [W, b])\n",
    "    # Update W and b following gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 11:21:51.335131: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [60000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-05-16 11:21:51.335599: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [60000,784]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<enumerate at 0x16be525c0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enumerate(train_data.take(5, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 784), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.uint8, name=None))>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[4 6 4 3 4 6 2 7 2 8 5 0 0 7 1 3 8 6 8 9 7 1 5 0 3 8 5 2 2 0 8 0 0 2 1 5 1\n",
      " 0 3 7 5 7 3 3 5 6 6 3 7 4 9 6 9 8 7 9 8 8 7 0 6 6 7 4 2 0 3 1 2 1 4 2 0 1\n",
      " 4 0 4 1 8 1 7 9 6 3 5 8 6 9 6 4 6 6 8 3 8 2 3 5 6 8 5 4 7 0 0 3 0 2 2 9 9\n",
      " 9 3 0 2 7 1 7 2 7 4 2 5 4 1 0 0 5 5 1 0 7 8 1 8 9 1 6 1 3 8 3 1 4 2 3 7 1\n",
      " 1 9 2 7 2 7 0 2 0 8 6 3 6 4 4 4 8 2 2 6 6 7 9 9 9 3 7 1 6 1 2 1 6 3 4 1 1\n",
      " 8 5 6 9 1 1 0 7 6 4 3 1 7 4 1 2 1 9 0 8 0 4 3 6 1 7 2 3 5 3 2 6 9 2 5 0 1\n",
      " 0 7 3 8 8 5 1 6 7 4 9 6 9 4 1 9 7 4 2 8 3 2 6 8 1 4 3 2 5 8 2 4 7 2], shape=(256,), dtype=uint8)\n",
      "tf.Tensor(\n",
      "[2 3 3 9 8 6 2 7 6 7 0 1 6 4 0 2 5 2 7 3 4 3 6 1 9 8 4 5 6 6 2 9 3 5 9 8 9\n",
      " 8 7 7 7 3 6 5 3 6 7 9 2 2 7 5 5 7 2 0 3 5 3 0 7 9 1 1 9 3 1 8 1 8 9 7 3 1\n",
      " 1 6 4 2 1 8 6 2 1 9 0 5 5 9 4 4 1 4 4 3 4 3 7 8 9 2 3 6 1 7 9 6 4 4 4 7 2\n",
      " 7 3 9 4 1 2 6 5 4 4 4 2 8 9 3 0 4 4 0 4 3 8 1 4 9 4 6 6 1 9 1 1 8 7 6 1 0\n",
      " 6 2 7 5 4 0 6 3 2 9 4 1 7 6 2 3 9 8 4 0 9 4 2 1 1 6 8 5 8 2 8 3 9 0 7 5 5\n",
      " 3 8 6 0 1 4 8 6 0 7 9 2 1 9 0 2 3 1 3 4 2 7 8 5 4 0 3 3 2 3 1 9 6 4 0 7 9\n",
      " 8 4 1 7 2 6 0 9 0 2 0 2 9 2 3 2 4 8 5 8 3 7 6 5 2 2 2 6 9 4 9 7 8 6], shape=(256,), dtype=uint8)\n",
      "tf.Tensor(\n",
      "[1 6 0 9 2 9 6 7 3 7 6 1 1 5 1 1 7 2 1 5 3 4 9 2 6 9 0 6 7 8 2 3 7 4 5 3 6\n",
      " 4 7 4 1 7 2 1 0 2 9 3 0 1 4 8 0 3 4 2 8 3 4 7 1 9 5 3 5 7 2 7 1 1 8 3 9 1\n",
      " 2 9 0 4 8 4 7 1 6 4 7 7 8 8 7 2 8 4 3 8 8 8 0 9 5 9 6 9 0 4 9 2 5 5 6 2 5\n",
      " 9 6 8 8 1 7 5 0 6 4 8 6 9 4 0 0 0 8 1 1 6 6 2 7 7 3 1 9 4 4 8 7 7 0 3 7 7\n",
      " 7 6 4 2 2 4 9 4 0 6 7 8 8 8 0 3 7 9 3 1 4 5 2 1 8 4 6 7 4 3 8 2 6 8 0 0 9\n",
      " 1 7 3 1 7 1 3 5 8 2 4 5 9 1 2 0 4 1 3 4 1 0 4 4 6 3 1 7 3 7 9 3 1 4 7 8 4\n",
      " 8 2 8 5 9 3 5 4 7 6 4 9 6 3 6 0 3 5 2 4 5 6 7 8 6 6 2 2 6 0 6 8 8 6], shape=(256,), dtype=uint8)\n",
      "tf.Tensor(\n",
      "[5 9 6 0 1 8 7 9 5 8 6 7 5 9 5 1 0 9 1 2 1 2 6 7 0 1 5 0 7 2 1 3 7 9 8 2 1\n",
      " 3 4 2 6 1 3 3 6 1 2 2 8 6 2 7 0 4 6 0 2 6 7 9 1 0 9 4 4 5 3 6 5 8 7 8 1 7\n",
      " 7 1 4 0 1 2 2 4 0 7 2 7 9 5 7 1 4 6 1 5 0 1 2 8 8 2 9 0 9 4 3 0 6 6 7 5 4\n",
      " 3 2 3 1 4 3 2 1 0 7 2 1 6 3 8 0 8 2 5 4 3 9 8 8 2 2 5 3 5 4 7 5 7 9 1 4 5\n",
      " 2 0 8 4 7 2 8 1 9 7 3 6 0 5 4 5 9 4 4 9 1 6 7 5 5 2 4 6 9 7 2 9 6 6 8 9 3\n",
      " 4 5 0 4 2 2 1 3 6 6 3 1 5 8 5 4 1 7 2 3 2 9 0 8 4 0 5 7 4 8 9 3 7 7 9 8 6\n",
      " 0 4 4 8 1 7 0 1 4 1 0 8 1 3 3 9 2 4 0 9 9 1 3 2 2 0 4 0 0 8 9 0 6 3], shape=(256,), dtype=uint8)\n",
      "tf.Tensor(\n",
      "[6 1 1 3 9 8 5 6 5 4 7 8 9 1 5 5 5 9 5 0 7 8 2 6 8 8 7 9 5 8 0 3 6 3 8 1 9\n",
      " 4 4 0 0 1 8 1 6 4 2 5 6 5 7 0 8 6 5 8 9 0 7 5 3 6 8 8 5 4 4 3 9 0 4 2 1 0\n",
      " 1 7 2 6 2 0 2 2 2 0 5 2 2 1 6 6 2 0 7 6 6 6 8 6 2 2 4 6 8 6 2 4 3 3 7 9 7\n",
      " 6 0 7 6 3 8 1 4 5 5 9 4 0 3 1 5 5 2 1 9 0 5 9 4 8 2 9 2 1 4 5 6 9 0 9 7 0\n",
      " 2 1 7 9 3 7 7 5 2 5 7 1 5 6 7 5 4 1 0 4 3 3 6 0 8 5 9 1 8 7 4 6 7 6 1 2 7\n",
      " 5 1 3 7 1 5 9 1 3 6 1 8 3 1 5 3 7 1 9 2 8 2 6 5 6 6 1 0 3 6 2 0 3 5 5 4 8\n",
      " 9 2 7 1 7 5 5 1 8 0 3 5 4 0 7 3 9 6 6 2 0 0 1 8 2 4 1 1 1 7 3 4 9 2], shape=(256,), dtype=uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 11:20:10.367792: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [60000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-05-16 11:20:10.368220: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [60000,784]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "for image, labels in train_data.take(5):\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 11:22:01.941207: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [60000,784]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-16 11:22:01.941699: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [60000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 614.903625, accuracy: 0.460938\n",
      "step: 50, loss: 595.460510, accuracy: 0.773438\n",
      "step: 100, loss: 624.926392, accuracy: 0.785156\n",
      "step: 150, loss: 70.205956, accuracy: 0.941406\n",
      "step: 200, loss: 207.158386, accuracy: 0.824219\n",
      "step: 250, loss: 67.996414, accuracy: 0.929688\n",
      "step: 300, loss: 90.952713, accuracy: 0.917969\n",
      "step: 350, loss: 158.530014, accuracy: 0.843750\n",
      "step: 400, loss: 163.232483, accuracy: 0.855469\n",
      "step: 450, loss: 25.215725, accuracy: 0.960938\n",
      "step: 500, loss: 114.201691, accuracy: 0.878906\n",
      "step: 550, loss: 53.076317, accuracy: 0.941406\n",
      "step: 600, loss: 92.225449, accuracy: 0.898438\n",
      "step: 650, loss: 75.926262, accuracy: 0.914062\n",
      "step: 700, loss: 61.724026, accuracy: 0.925781\n",
      "step: 750, loss: 202.819153, accuracy: 0.847656\n",
      "step: 800, loss: 85.210052, accuracy: 0.917969\n",
      "step: 850, loss: 45.227894, accuracy: 0.933594\n",
      "step: 900, loss: 154.704849, accuracy: 0.835938\n",
      "step: 950, loss: 98.358047, accuracy: 0.902344\n"
     ]
    }
   ],
   "source": [
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps, 0)):\n",
    "    run_optimization(batch_x, batch_y)\n",
    "    if step % display_step == 0:\n",
    "        pred = logistic_regression(batch_x)\n",
    "        loss = cross_entropy(pred, batch_y)\n",
    "        acc = accuracy(pred, batch_y)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.839900\n"
     ]
    }
   ],
   "source": [
    "pred = logistic_regression(x_test)\n",
    "\n",
    "print(\"Test Accuracy: %f\" % accuracy(pred, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
